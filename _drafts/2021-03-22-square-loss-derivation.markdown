---
title: "Machine Learning Loss Math Cheat Sheet"
date: 2021-03-21 12:00:00 -0700
comments: true
author: "Will High"
categories: 
  - machine learning
tags:
  - loss function
excerpt: Derive losses from likelihood plus optional regularization priors.
---

{% include toc %}

# tl;dr 

Cheat sheet for deriving loss functions from a data generating model likelihood
and optional priors. This post is different from other cheat sheets because it
includes more model types than I have typically seen in one place,
including survival and quantile models. 

# Bayes theorem

Bayes' theorem tells us that the posterior probability of a hypothesis $h$ given data $D$ is

\begin{equation}
P(H|D) = \frac{P(H) P(D|H)}{P(D)},
\end{equation}

where

* $P(H \vert D)$ is the **posterior** probability of the (variable) hypothesis given the (fixed) observed data
* $P(H)$ is the **prior** probability of the hypothesis
* $P(D \vert H)$ is the probability that the observed data was generated by $H$, aka the **likelihood**
* $P(D)$ is the marginal likelihood, usually discarded because it's not a function of $H$.

In supervised machine learning,
models are hypotheses
and data are
$y_i | \mathbf{x}_i$ label-feature vector tuples.

We're looking for the best model, which maximizes the posterior probability. 

\begin{equation}
h_{\textrm{best}} = \textrm{argmax}_h p(h) p({y | \mathbf{x}} | h)
\end{equation}

# Stochastic gradient descent and gradient boosting

TBW

# Likelihood, loss, gradient, hessian

The loss is the negative log-likelihood for a single data point.

## Square loss

Used in continous variable regression problems. 

Likelihood function, called Gaussian or normal:

\begin{equation}
\prod_{i=1}^N\frac{1}{\sigma\sqrt{2\pi}}\exp{-\frac{(y_i - h(\mathbf{x}_i))^2}{2\sigma^2}}
\end{equation}

Loss function:

\begin{equation}
(y_i - h(\mathbf{x}_i))^2
\end{equation}

Gradient:

\begin{equation}
(y_i - h(\mathbf{x}_i))^2
\end{equation}


## Log loss

Used in binary classifiction problems. 

Likelihood function:

Bernoulli

\begin{equation}
\prod_{i=1}^N p(\mathbf{x}_i)^{y_i} (1 - p(\mathbf{x}_i))^{1 - {y_i}}
\end{equation}

The hypothesis in this case is the log-odds, which is a function 
with support $h \in \\{-\infty, \infty\\}$.

\begin{equation} 
h(\mathbf{x}_i) = \log{\frac{p(\mathbf{x}_i)}{1 - p(\mathbf{x}_i)}}
\end{equation}

This formulation maps the boundless hypotheses 
onto probabilities $p \in \\{0, 1\\}$ by just solving for $p$:

\begin{equation} 
p(\mathbf{x}_i) = \frac{1}{1 + \exp{(-h(\mathbf{x}_i))}}
\end{equation}


Loss function

For labels following the binary indicator convention $y \in \{0, 1\}$:

$$\begin{eqnarray}
\ell & = & -y_i\log{p(\mathbf{x}_i)} - (1 - y_i)\log{(1 - p(\mathbf{x}_i))} \\
  & = & abc
\end{eqnarray}$$

For labels following the transformed convention $z = 2y-1 \in \{-1, 1\}$:

\begin{equation}
\log{h(\mathbf{x}_i)} - (1 - y_i)\log{(1 - h(\mathbf{x}_i))}
\end{equation}


## Cox proportional hazards survival

Likelihood function: Cox partial likelihood

Loss function

Gradient

Hessian

## Weibull survival


## More survival


## Quantile regression

Likelihood function

Loss function

Gradient

Hessian

## Mean absolute deviation

Mean absolute deviation is quantile regression at $\alpha=0.5$.

## Hinge



## Huber


## Poisson


## Kullback-Leibler







# Regularizing priors


# Further reading


* [R GBM vignette, Section 4 "Available Distributions"](https://cran.r-project.org/web/packages/gbm/vignettes/gbm.pdf)
* [ML Cheat Sheet, Section "Loss Functions"](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)
* [Supervised Learning cheatsheet](https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-supervised-learning)
* [Stochastic Gradient Descent Tricks](https://www.microsoft.com/en-us/research/wp-content/uploads/2012/01/tricks-2012.pdf)

