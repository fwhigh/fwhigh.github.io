---
title: "Square Loss Derivation Cheat Sheet"
date: 2021-03-21 12:00:00 -0700
comments: true
author: "Will High"
categories: 
  - machine learning
tags:
  - loss function
excerpt: Derive the square loss from a Gaussian likelihood, with optional regularization prior.
---

{% include toc %}

# tl;dr 

Cheat sheet for deriving square loss from a Gaussian error model with optional priors 
that reproduce Ridge (L2) and LASSO (L1) regularization.

# Bayes theorem

Bayes' theorem tells us that the posterior probability of a hypothesis $h$ given data $D$ is

\begin{equation}
P(H|D) = \frac{P(H) P(D|H)}{P(D)},
\end{equation}

where

* $P(H \vert D)$ is the **posterior** probability of the (variable) hypothesis given the (fixed) observed data
* $P(H)$ is the **prior** probability of the hypothesis
* $P(D \vert H)$ is the probability that the observed data was generated by $H$, aka the **likelihood**
* $P(D)$ is the marginal likelihood, usually discarded because it's not a function of $H$

In supervised machine learning,
models are hypotheses
and data are
$Y_i | \mathbf{x}_i$ label-feature vector tuples.

# Log-likelihood

We're looking for the best model, which maximizes the posterior probability. 

\begin{equation}
h_{\textrm{best}} = \textrm{argmax}_h p(h) p({Y | \mathbf{x}} | h)
\end{equation}